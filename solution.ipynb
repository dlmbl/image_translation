{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20d5d8ad",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 2
   },
   "source": [
    "# A Generative Modelling Approach to Image translation\n",
    "---\n",
    "\n",
    "Written by Samuel Tonks, Krull Lab, University of Birmingham, UK.<br><br>\n",
    "\n",
    "In this part of the exercise, we will approach the same supervised image-to-image translation task as in the previous parts, but using a different model architecture. Here we will explore a generative modelling approach; a conditional Generative Adversarial Network (cGAN). <br>\n",
    "\n",
    "In contrast to formulating the task as a regression problem where the model produces a single deterministic output, cGANs learn to map from the source domain to a target domain distribution. This learnt distribution can then be sampled from to produce virtual staining predictions that are no longer a compromise between possible solutions which can lead to improved sharpness and realism in the generated images.<br>\n",
    "\n",
    "At a high-level a cGAN has two networks; a generator and a discriminator. The generator is a fully convolutional network that takes the source image as input and outputs the target image. The discriminator is also a fully convolutional network that takes as input the source image concatentated with a real or fake image and outputs the probabilities of whether the image is real or fake as shown in the Figure below:\n",
    "\n",
    "![Overview of cGAN](https://github.com/Tonks684/dlmbl_material/tree/main/imgs/GAN.jpg)\n",
    "<br>\n",
    "The generator is trained to fool the discriminator into predicting a high probability that its generated outputs are real, and the discriminator is trained to distinguish between real and fake images. Both networks are trained using an adversarial loss in a min-max game, where the generator tries to minimize the probability of the discriminator correctly classifying its outputs as fake, and the discriminator tries to maximize this probability. It is typically trained until the discriminator can no longer determine whether or not the generated images are real or fake better than a random guess (p(0.5)).<br>\n",
    "\n",
    "We will be exploring [Pix2PixHD GAN](https://arxiv.org/abs/1711.11585) architecture, a high-resolution extension of a traditional cGAN adapted for our recent [virtual staining works](https://ieeexplore.ieee.org/abstract/document/10230501?casa_token=NEyrUDqvFfIAAAAA:tklGisf9BEKWVjoZ6pgryKvLbF6JyurOu5Jrgoia1QQLpAMdCSlP9gMa02f3w37PvVjdiWCvFhA). Pix2PixHD GAN improves upon the traditional cGAN by using a coarse-to-fine generator, a multi-scale discrimator and additional loss terms. The \"coarse-to-fine\" generator is composed of two sub-networks, both ResNet architectures that operate at different scales. The first sub-network (G1) generates a low-resolution image, which is then upsampled and concatenated with the source image to produce a higher resolution image. The multi-scale discriminator is composed of 3 networks that operate at different scales, each network is trained to distinguish between real and fake images at that scale. The generator is trained to fool the discriminator at each scale. The additional loss terms include a feature matching loss, which encourages the generator to produce images that are similar to the real images at each scale. <br>\n",
    "![Pix2PixGAN ](https://github.com/Tonks684/image_translation/tree/main/imgs/Pix2PixHD_1.jpg)\n",
    "![Feature Matching Loss Pix2PixHD GAN](https://github.com/Tonks684/image_translation/tree/main/imgs/Pix2PixHD_2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e7de22",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Today, we will train a 2D image translation model using the Pix2PixHD GAN. We will use the same dataset of 301 fields of view (FOVs) of Human Embryonic Kidney (HEK) cells, each FOV has 3 channels (phase, membrane, and nuclei) as used in the previous section.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd644c2",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "This part of the exercise is organized in 3 parts.<br>\n",
    "\n",
    "As you have already explored the data in the previous parts, we will focus on training and evaluating Pix2PixHD GAN. The parts are as follows:<br>\n",
    "\n",
    "* **Part 1** - Define dataloaders & walk through steps to train a Pix2PixHD GAN.<br>\n",
    "* **Part 2** - Load and assess a pre-trained Pix2PixGAN using tensorboard, discuss the different loss components and how new hyper-parameter configurations could impact performance.<br>\n",
    "* **Part 3** - Evaluate performance of pre-trained Pix2PixGAN using pixel-level and instance-level metrics.<br>\n",
    "* **Part 4** - Compare the performance of Viscy (regression-based) with Pix2PixHD GAN (generative modelling approach)<br>\n",
    "* **Part 5** - BONUS: Sample different virtual staining solutions from the GAN using MC-Dropout and explore the uncertainty in the virtual stain predictions.<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be458108",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "Our guesstimate is that each of the parts will take ~1 hour. A reasonable Pix2PixHD GAN can be trained in ~1.5 hours on a typical AWS node, this notebook is designed to walk you through the training steps but load a pre-trained model and tensorboard session to ensure we can complete the exercise in the time allocated. During Part 2 or 3, you're free to train your own model using the steps we outline in part 1.<br>\n",
    "\n",
    "The focus of this part of the image_translation session is on understanding a generative modelling approach to image translation, how to train and measure training performance for Pix2PixHD GAN, exploring pixel-level and instance-level metrics for evaluating the performance of the model. In the final section we will compare and discuss the Viscy vs Pix2PixHD GAN results. There is also a bonus part 4 where you can explore the variability in the samples from Pix2PixHD's generator. <br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb128b5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "Set your python kernel to <span style=\"color:black;\">04_image_translation_phd</span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecba003b",
   "metadata": {
    "cell_marker": "\"\"\"",
    "title": "<a ></a>"
   },
   "source": [
    "# Part 1: Define dataloaders & walk through steps to train a Pix2PixHD GAN.\n",
    "---------\n",
    "The focus of this part of the exercise is on understanding a generative modelling approach to image translation, how to train and evaluate a cGAN, and explore some hyperparameters of the cGAN. \n",
    "\n",
    "Learning goals:\n",
    "\n",
    "- Load dataset and configure dataloader.\n",
    "- Configure Pix2PixHD GAN and train to predict nuclei from phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a0d139f",
   "metadata": {
    "title": "Imports and paths"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-07-22 10:58:36.589155: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-22 10:58:37.773733: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/site-packages/cv2/../../lib64:/cm/shared/slurm/current/lib/slurm:/cm/shared/slurm/current/lib:/cm/shared/slurm/current/lib/slurm:/cm/shared/slurm/current/lib\n",
      "2024-07-22 10:58:37.773858: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/site-packages/cv2/../../lib64:/cm/shared/slurm/current/lib/slurm:/cm/shared/slurm/current/lib:/cm/shared/slurm/current/lib/slurm:/cm/shared/slurm/current/lib\n",
      "2024-07-22 10:58:37.773868: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from skimage import metrics\n",
    "from tifffile import imread, imsave\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import all the necessary hyperparameters and configurations for training.\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.options.train_options import TrainOptions\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.options.test_options import TestOptions\n",
    "\n",
    "# Import Pytorch dataloader and transforms.\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.data.data_loader_dlmbl import CreateDataLoader\n",
    "\n",
    "# Import the model architecture.\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.models import create_model\n",
    "\n",
    "# Import helper functions for visualization and processing.\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.util.visualizer import Visualizer\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.util import util\n",
    "\n",
    "# Import train script.\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.train_dlmbl import train as train_model\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.test_dlmbl import inference as inference_model\n",
    "from GAN_code.GANs_MI2I.pix2pixHD.test_dlmbl import sampling\n",
    "\n",
    "# Import the function to compute segmentation scores.\n",
    "from GAN_code.GANs_MI2I.segmentation_scores import gen_segmentation_scores\n",
    "# pytorch lightning wrapper for Tensorboard.\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Initialize the default options and parse the arguments.\n",
    "opt = TrainOptions().parse()\n",
    "# Set the seed for reproducibility.\n",
    "util.set_seed(42)\n",
    "# Set the experiment folder name.\n",
    "translation_task = \"nuclei\"  # or \"cyto\" depending on your choice of target for virtual stain.\n",
    "opt.name = \"dlmbl_vsnuclei\"\n",
    "# Path to store all the logs.\n",
    "opt.checkpoints_dir = Path(f\"./training/\").expanduser()\n",
    "output_image_folder = Path(\"./data/04_image_translation/tiff_files/\").expanduser()\n",
    "# Initalize the tensorboard writer.\n",
    "writer = SummaryWriter(log_dir=opt.checkpoints_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ca473f",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## 1.1 Load Dataset & Configure Dataloaders.<br>\n",
    "Having already downloaded and split our training, validation and test sets we now need to load the data into the model. We will use the Pytorch DataLoader class to load the data in batches. The DataLoader class is an iterator that provides a consistent way to load data in batches. We will also use the CreateDataLoader class to load the data in the correct format for the Pix2PixHD GAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d344b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEKCells Data Loader\n",
      "HEKCells Dataset dataset was created\n",
      "Total Training Images = 2764\n",
      "HEKCells Data Loader\n",
      "HEKCells Dataset dataset was created\n",
      "Total Validation Images = 692\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Dataset and Dataloaders.\n",
    "\n",
    "## Define Dataset & Dataloader options.\n",
    "opt.dataroot = output_image_folder\n",
    "opt.data_type = 16  # Data type of the images.\n",
    "opt.loadSize = 512  # Size of the loaded phase image.\n",
    "opt.input_nc = 1  # Number of input channels.\n",
    "opt.output_nc = 1  # Number of output channels.\n",
    "opt.resize_or_crop = \"none\"  # Scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop|none].\n",
    "opt.target = \"nuclei\"  # or \"cyto\" depending on your choice of target for virtual stain.\n",
    "\n",
    "# Load Training Set for input into model\n",
    "train_dataloader = CreateDataLoader(opt)\n",
    "dataset_train = train_dataloader.load_data()\n",
    "print(f\"Total Training Images = {len(train_dataloader)}\")\n",
    "\n",
    "# Load Val Set\n",
    "opt.phase = \"val\"\n",
    "val_dataloader = CreateDataLoader(opt)\n",
    "dataset_val = val_dataloader.load_data()\n",
    "print(f\"Total Validation Images = {len(val_dataloader)}\")\n",
    "opt.phase= \"train\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae515f4e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "## Configure Pix2PixHD GAN and train to predict nuclei from phase.\n",
    "Having loaded the data into the model we can now train the Pix2PixHD GAN to predict nuclei from phase. We will use the following hyperparameters to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f8e2027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Options -------------\n",
      "batchSize: 8\n",
      "beta1: 0.5\n",
      "checkpoints_dir: training\n",
      "continue_train: False\n",
      "data_type: 16\n",
      "dataroot: data/04_image_translation/tiff_files\n",
      "debug: False\n",
      "display_freq: 100\n",
      "display_winsize: 512\n",
      "dropout_variation_inf: False\n",
      "feat_num: 3\n",
      "fineSize: 512\n",
      "float: True\n",
      "fp16: False\n",
      "gpu_ids: [0]\n",
      "input_RGB: False\n",
      "input_nc: 1\n",
      "instance_feat: False\n",
      "isTrain: True\n",
      "label_feat: False\n",
      "label_nc: 0\n",
      "lambda_feat: 10.0\n",
      "loadSize: 512\n",
      "load_features: False\n",
      "load_pretrain: \n",
      "local_rank: 0\n",
      "lr: 0.0002\n",
      "max_dataset_size: inf\n",
      "model: pix2pixHD\n",
      "nThreads: 2\n",
      "n_blocks_global: 9\n",
      "n_blocks_local: 3\n",
      "n_clusters: 10\n",
      "n_downsample_E: 4\n",
      "n_downsample_global: 4\n",
      "n_layers_D: 3\n",
      "n_local_enhancers: 1\n",
      "name: dlmbl_vsnuclei\n",
      "ndf: 32\n",
      "nef: 16\n",
      "netG: global\n",
      "ngf: 64\n",
      "niter: 100\n",
      "niter_decay: 100\n",
      "niter_fix_global: 0\n",
      "no_flip: True\n",
      "no_ganFeat_loss: False\n",
      "no_html: False\n",
      "no_instance: True\n",
      "no_lsgan: False\n",
      "no_vgg_loss: True\n",
      "norm: instance\n",
      "num_D: 3\n",
      "output_RGB: False\n",
      "output_nc: 1\n",
      "output_reshape: None\n",
      "phase: train\n",
      "pool_size: 0\n",
      "print_freq: 100\n",
      "resize_or_crop: none\n",
      "save_epoch_freq: 20\n",
      "save_latest_freq: 1000\n",
      "seed: None\n",
      "serial_batches: False\n",
      "target: nuclei\n",
      "tf_log: False\n",
      "use_dropout: \n",
      "verbose: False\n",
      "which_epoch: latest\n",
      "-------------- End ----------------\n",
      "GlobalGenerator(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (17): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (18): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (19): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (20): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (21): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (22): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (23): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (24): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (30): ReLU(inplace=True)\n",
      "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (38): Conv2d(64, 1, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (39): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30455/3172822193.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# Initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mphase2nuclei_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;31m# Define Optimizers for G and D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m optimizer_G, optimizer_D = (\n",
      "\u001b[0;32m/hpc/projects/upt/samuel_tonks_experimental_space/dlmbl/dlmbl_material/GAN_code/GANs_MI2I/pix2pixHD/models/models.py\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m(opt)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mui_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUIModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUIModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model [%s] was created'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/projects/upt/samuel_tonks_experimental_space/dlmbl/dlmbl_material/GAN_code/GANs_MI2I/pix2pixHD/models/pix2pixHD_model.py\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, opt)\u001b[0m\n\u001b[1;32m     63\u001b[0m         self.netG = networks.define_G(netG_input_nc, opt.output_nc, opt.ngf, opt.netG,\n\u001b[1;32m     64\u001b[0m                                       \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_downsample_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_blocks_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_local_enhancers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                                       opt.n_blocks_local, opt.dropout_variation_inf,opt.norm, gpu_ids=self.gpu_ids)\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;31m# Discriminator network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/projects/upt/samuel_tonks_experimental_space/dlmbl/dlmbl_material/GAN_code/GANs_MI2I/pix2pixHD/models/networks.py\u001b[0m in \u001b[0;36mdefine_G\u001b[0;34m(input_nc, output_nc, ngf, netG, n_downsample_global, n_blocks_global, n_local_enhancers, n_blocks_local, use_dropout, norm, gpu_ids)\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpu_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mnetG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights_init\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnetG\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mcuda\u001b[0;34m(self, device)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    357\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    461\u001b[0m             \u001b[0mModule\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \"\"\"\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the parameters for the Generator.\n",
    "opt.ngf = 64  # Number of filters in the generator.\n",
    "opt.n_downsample_global = 4  # Number of downsampling layers in the generator.\n",
    "opt.n_blocks_global = 9  # Number of residual blocks in the generator.\n",
    "opt.n_blocks_local = 3  # Number of residual blocks in the generator.\n",
    "opt.n_local_enhancers = 1  # Number of local enhancers in the generator.\n",
    "\n",
    "# Define the parameters for the Discriminators.\n",
    "opt.num_D = 3  # Number of discriminators.\n",
    "opt.n_layers_D = 3  # Number of layers in the discriminator.\n",
    "opt.ndf = 32  # Number of filters in the discriminator.\n",
    "\n",
    "# Define general training parameters.\n",
    "opt.gpu_ids = [0] # GPU ids to use.\n",
    "opt.norm = \"instance\"  # Normalization layer in the generator.\n",
    "opt.use_dropout = \"\"  # Use dropout in the generator (fixed at 0.2).\n",
    "opt.batchSize = 8  # Batch size.\n",
    "\n",
    "# Create a visualizer to perform image processing and visualization\n",
    "visualizer = Visualizer(opt)\n",
    "\n",
    "\n",
    "# Here will first start training a model from scrach however we can continue to train from a previously trained model by setting the following parameters.\n",
    "opt.continue_train = False\n",
    "if opt.continue_train:\n",
    "    iter_path = os.path.join(opt.checkpoints_dir, opt.name, \"iter.txt\")\n",
    "    try:\n",
    "        start_epoch, epoch_iter = np.loadtxt(iter_path, delimiter=\",\", dtype=int)\n",
    "    except:\n",
    "        start_epoch, epoch_iter = 1, 0\n",
    "    print(\"Resuming from epoch %d at iteration %d\" % (start_epoch, epoch_iter))\n",
    "else:\n",
    "    start_epoch, epoch_iter = 1, 0\n",
    "    \n",
    "print('------------ Options -------------')\n",
    "for k, v in sorted(vars(opt).items()):\n",
    "    print('%s: %s' % (str(k), str(v)))\n",
    "print('-------------- End ----------------')\n",
    "\n",
    "# Initialize the model\n",
    "phase2nuclei_model = create_model(opt)\n",
    "# Define Optimizers for G and D\n",
    "optimizer_G, optimizer_D = (\n",
    "    phase2nuclei_model.module.optimizer_G,\n",
    "    phase2nuclei_model.module.optimizer_D,\n",
    ")\n",
    "\n",
    "train_model(\n",
    "    opt,\n",
    "    phase2nuclei_model,\n",
    "    visualizer,\n",
    "    dataset_train,\n",
    "    dataset_val,\n",
    "    optimizer_G,\n",
    "    optimizer_D,\n",
    "    start_epoch,\n",
    "    epoch_iter,\n",
    "    writer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce20ba8e",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## A heads up of what to expect from the training...\n",
    "<br>\n",
    "The train_model function has been designed so you can see the different Pix2PixHD GAN loss components discussed in the first part of the exercise as well as additional performance measurements.<br> \n",
    "As previously mentioned, Pix2PixHD GAN has two networks; a generator and a discriminator. The generator is trained to fool the discriminator into predicting a high probability that its generated outputs are real, and the discriminator is trained to distinguish between real and fake images. Both networks are trained using an adversarial loss in a min-max game, where the generator tries to minimize the probability of the discriminator correctly classifying its outputs as fake, and the discriminator tries to maximize this probability. It is typically trained until the discriminator can no longer determine whether or not the generated images are real or fake better than a random guess (p(0.5)). After a we have iterated through all the training data, we validate the performance of the network on the validation dataset. <br>\n",
    "\n",
    "In light of this, we plot the discriminator probabilities of real (D_real) and fake (D_fake) images, for the training and validation datasets.<br>\n",
    "\n",
    "Both networks are also trained using the feature matching loss (Generator_GAN_Loss_Feat), which encourages the generator to produce images that contain similar statistics to the real images at each scale. We also plot the feature matching L1 loss for the training and validation sets together to observe the performance and how the model is fitting the data.<br>\n",
    "\n",
    "In our implementation, in addition to the Pix2PixHD GAN loss components already described we stabalize the GAN training by additing an additional least-square loss term. This term stabalizes the training of the GAN by penalizing the generator for producing images that the discriminator is very confident (high probability) are fake. This loss term is added to the generator loss and is used to train the generator to produce images that are similar to the real images.\n",
    "\n",
    "We plot the least-square loss (Generator_Loss_GAN) for the training and validation sets together to observe the performance and how the model is fitting the data.<br>\n",
    "This implementation allows for the turning on/off of the least-square loss term by setting the --no_lsgan flag to the model options. As well as the turning off of the feature matching loss term by setting the --no_ganFeat_loss flag to the model options. Something you might want to explore in the next section!<br><br>\n",
    "\n",
    "Finally, we also plot the Peak-Signal-to-Noise-Ratio (PSNR) and the Structural Similarity Index Measure (SSIM) for the training and validation sets together to observe the performance and how the model is fitting the data.<br>\n",
    "\n",
    "[PSNR](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio), is a widely used metric to assess the quality of the generated image compared to the target image. Formally. it measures the ratio between the maximum possible power of a signal and the power of the corrupting noise that affects the fidelity of its representation. Essentially, PSNR provides a quantitative measurement of the quality of an image after compression or other processing such as image translation. Unlike the Pearson-Coeffecient, when measuring how much the pixel values of the virtual stain deviate from the target nuceli stain the score is sensitive to changes in brightness and contrast which is required for necessary for evaluating virtual staining. PSNR values range from 0dB to upper bounds that rarely exceed 60 dB. Extremely high PSNR values (above 50 dB) typically indicate almost negligible differences between the images.<br>\n",
    "\n",
    "\n",
    "[SSIM](https://en.wikipedia.org/wiki/Structural_similarity), is a perceptual metric used to measure the similarity between two images. Unlike PSNR, which focuses on pixel-wise differences, SSIM evaluates image quality based on perceived changes in structural information, luminance, and contrast. SSIM values range from -1 to 1, where 1 indicates perfect similarity between the images. SSIM is a more robust metric than PSNR, as it takes into account the human visual system\"s sensitivity to structural information and contrast. SSIM is particularly useful for evaluating the quality of image translation models, as it provides a more accurate measure of the perceptual similarity between the generated and target images.<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c074ea",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0,
    "title": "<a ></a>"
   },
   "source": [
    "\n",
    "\n",
    "# Part 2: Load & Assess trained Pix2PixGAN using tensorboard, discuss performance of the model.\n",
    "--------------------------------------------------\n",
    "Learning goals:\n",
    "- Understand the loss components of Pix2PixHD GAN and how they are used to train the model.\n",
    "- Evaluate the fit of the model on the train and validation datasets.\n",
    "\n",
    "In this part, we will evaluate the performance of the pre-trained model as shown in the previous part. We will begin by looking qualitatively at the model predictions, then dive into the different loss curves, as well as the SSIM and PSNR scores achieves on the validation set. We will also train another model to see if we can improve the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e0bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = f\"./GAN_code/GANs_MI2I/pre_trained/{opt.name}/\"\n",
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir $log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be90b05",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Qualitative evaluation:\n",
    "<br>\n",
    "We have visualised the model output for an unseen phase contrast image and the target, nuclei stain.<br>\n",
    "<br>\n",
    "- What do you notice about the virtual staining predictions? Are they realistic? How does the sharpness and visual representation compare to the regression-based approach?<br>\n",
    "- What do you notice about the translation of the background pixels compared the translation of the instance pixels?\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Quantitative evaluation:\n",
    "<br>\n",
    "- What do you notice about the probabilities (real vs fake) of the discriminators?How do the values compare during training compared to validation?<br>\n",
    "- What do you notice about the feature matching L1 loss?<br>\n",
    "- What do you notice about the least-square loss?<br>\n",
    "- What do you notice about the PSNR and SSIM scores? Are we over or underfitting at all?<br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108610e5",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "## Checkpoint 1\n",
    "\n",
    "Congratulations! You should now have a better understanding of how a conditional generative model works! Please feel in your own time to train your own Pix2PixHD GAN model and evaluate the performance of the training of the model.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03b9c66",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "# Part 3: Evaluate performance of the virtual staining on unseen data.\n",
    "--------------------------------------------------\n",
    "## Evaluate the performance of the model.\n",
    "We now look at the same metrics of performance of the previous model. We typically evaluate the model performance on a held out test data. \n",
    "\n",
    "Steps:\n",
    "- Define our model parameters for the pre-trained model (these are the same parameters as shown in earlier cells but copied here for clarity).\n",
    "- Load the test data.\n",
    "\n",
    "We will first load the test data using the same format as the training and validation data. We will then use the model to predict the nuclei channel from the phase image. We will then evaluate the performance of the model using the following metrics:\n",
    "\n",
    "Pixel-level metrics:\n",
    "- [Peak-Signal-to-Noise-Ratio (PSNR)](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio).\n",
    "- [Structural Similarity Index Measure (SSIM)](https://en.wikipedia.org/wiki/Structural_similarity).\n",
    "\n",
    "Instance-level metrics:\n",
    "- [F1 score](https://en.wikipedia.org/wiki/F1_score). via [Cellpose](https://cellpose.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "472905aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HEKCells Data Loader\n",
      "HEKCells Dataset dataset was created\n",
      "GlobalGenerator(\n",
      "  (model): Sequential(\n",
      "    (0): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (1): Conv2d(1, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (5): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (8): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (9): ReLU(inplace=True)\n",
      "    (10): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (11): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (12): ReLU(inplace=True)\n",
      "    (13): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
      "    (14): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (17): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (18): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (19): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (20): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (21): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (22): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (23): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (24): ResnetBlock(\n",
      "      (conv_block): Sequential(\n",
      "        (0): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (2): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "        (3): ReLU(inplace=True)\n",
      "        (4): Dropout(p=0, inplace=False)\n",
      "        (5): ReflectionPad2d((1, 1, 1, 1))\n",
      "        (6): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1))\n",
      "        (7): InstanceNorm2d(1024, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "      )\n",
      "    )\n",
      "    (25): ConvTranspose2d(1024, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (26): InstanceNorm2d(512, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): ConvTranspose2d(512, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (29): InstanceNorm2d(256, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (30): ReLU(inplace=True)\n",
      "    (31): ConvTranspose2d(256, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (32): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): ConvTranspose2d(128, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), output_padding=(1, 1))\n",
      "    (35): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
      "    (36): ReLU(inplace=True)\n",
      "    (37): ReflectionPad2d((3, 3, 3, 3))\n",
      "    (38): Conv2d(64, 1, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (39): Tanh()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "opt = TestOptions().parse(save=False)\n",
    "\n",
    "# Define the parameters for the dataset.\n",
    "opt.dataroot = output_image_folder\n",
    "opt.data_type = 16  # Data type of the images.\n",
    "opt.loadSize = 512  # Size of the loaded phase image.\n",
    "opt.input_nc = 1  # Number of input channels.\n",
    "opt.output_nc = 1  # Number of output channels.\n",
    "opt.target = \"cyto\"  # \"nuclei\" or \"cyto\" depending on your choice of target for virtual stain.\n",
    "opt.resize_or_crop = \"none\"  # Scaling and cropping of images at load time [resize_and_crop|crop|scale_width|scale_width_and_crop|none].\n",
    "opt.batchSize = 1 # Batch size for training\n",
    "\n",
    "# Define the model parameters for the pre-trained model.\n",
    "\n",
    "# Define the parameters for the Generator.\n",
    "opt.ngf = 64  # Number of filters in the generator.\n",
    "opt.n_downsample_global = 4  # Number of downsampling layers in the generator.\n",
    "opt.n_blocks_global = 9  # Number of residual blocks in the generator.\n",
    "opt.n_blocks_local = 3  # Number of residual blocks in the generator.\n",
    "opt.n_local_enhancers = 1  # Number of local enhancers in the generator.\n",
    "\n",
    "# Define the parameters for the Discriminators.\n",
    "opt.num_D = 3  # Number of discriminators.\n",
    "opt.n_layers_D = 3  # Number of layers in the discriminator.\n",
    "opt.ndf = 32  # Number of filters in the discriminator.\n",
    "\n",
    "# Define general training parameters.\n",
    "opt.gpu_ids= [0]  # GPU ids to use.\n",
    "opt.norm = \"instance\"  # Normalization layer in the generator.\n",
    "opt.use_dropout = \"\"  # Use dropout in the generator (fixed at 0.2).\n",
    "opt.batchSize = 8  # Batch size.\n",
    "\n",
    "# Define loss functions.\n",
    "opt.no_vgg_loss = \"\"  # Turn off VGG loss\n",
    "opt.no_ganFeat_loss = \"\"  # Turn off feature matching loss\n",
    "opt.no_lsgan = \"\"  # Turn off least square loss\n",
    "\n",
    "# Additional Inference parameters\n",
    "opt.name = f\"dlmbl_vscyto\"\n",
    "opt.how_many = 112  # Number of images to generate.\n",
    "opt.checkpoints_dir = f\"./GAN_code/GANs_MI2I/pre_trained/\"  # Path to the model checkpoints.\n",
    "opt.results_dir = f\"./GAN_code/GANS_MI2I/pre_trained/{opt.name}/inference_results/\"  # Path to store the results.\n",
    "opt.which_epoch = \"latest\"  # or specify the epoch number \"40\"\n",
    "opt.phase = \"test\"\n",
    "\n",
    "opt.nThreads = 1  # test code only supports nThreads = 1\n",
    "opt.batchSize = 1  # test code only supports batchSize = 1\n",
    "opt.serial_batches = True  # no shuffle\n",
    "opt.no_flip = True  # no flip\n",
    "Path(opt.results_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load the test data.\n",
    "test_data_loader = CreateDataLoader(opt)\n",
    "test_dataset = test_data_loader.load_data()\n",
    "visualizer = Visualizer(opt)\n",
    "\n",
    "# Load pre-trained model\n",
    "model = create_model(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b29931d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 112/112 [00:14<00:00,  7.53it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate & save predictions in the results directory.\n",
    "inference_model(test_dataset, opt, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee40372c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "112it [00:00, 245.10it/s]\n"
     ]
    }
   ],
   "source": [
    "# Gather results for evaluation\n",
    "virtual_stain_paths = sorted([i for i in Path(opt.results_dir).glob(\"**/*.tiff\")])\n",
    "target_stain_paths = sorted([i for i in Path(f\"{output_image_folder}/{translation_task}/test/\").glob(\"**/*.tiff\")])\n",
    "phase_paths = sorted([i for i in Path(f\"{output_image_folder}/input/test/\").glob(\"**/*.tiff\")])\n",
    "assert (len(virtual_stain_paths) == len(target_stain_paths) == len(phase_paths)\n",
    "), \"Number of images do not match.\"\n",
    "\n",
    "# Create arrays to store the images.\n",
    "virtual_stains = np.zeros((len(virtual_stain_paths), 512, 512))\n",
    "target_stains = virtual_stains.copy()\n",
    "phase_images = virtual_stains.copy()\n",
    "# Load the images and store them in the arrays.\n",
    "for index, (v_path, t_path, p_path) in tqdm(\n",
    "    enumerate(zip(virtual_stain_paths, target_stain_paths, phase_paths))\n",
    "):\n",
    "    virtual_stain = imread(v_path)\n",
    "    phase_image = imread(p_path)\n",
    "    target_stain = imread(t_path)\n",
    "    # Append the images to the arrays.\n",
    "    phase_images[index] = phase_image\n",
    "    target_stains[index] = target_stain\n",
    "    virtual_stains[index] = virtual_stain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf914f3",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 1,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.1 Visualise the results of the model on the test set.\n",
    "\n",
    "Create a matplotlib plot that visalises random samples of the phase images, target stains, and virtual stains.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dfb596",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "######## TODO ########\n",
    "##########################\n",
    "\n",
    "def visualise_results():\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1aa4081",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "##########################\n",
    "######## Solution ########\n",
    "##########################\n",
    "\n",
    "def visualise_results(\n",
    "    phase_images: np.array, target_stains: np.array, virtual_stains: np.array, crop_size=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes the results of the image processing algorithm.\n",
    "\n",
    "    Args:\n",
    "        phase_images (np.array): Array of phase images.\n",
    "        target_stains (np.array): Array of target stain images.\n",
    "        virtual_stains (np.array): Array of virtual stain images.\n",
    "        crop_size (int, optional): Size of the crop. Defaults to None.\n",
    "    \"\"\"\n",
    "\n",
    "    fig, axes = plt.subplots(5, 3, figsize=(15, 20))\n",
    "    sample_indices = np.random.choice(len(phase_images), 5)\n",
    "    if crop_size is not None:\n",
    "        phase_images = phase_images[:,:crop_size,:crop_size]\n",
    "        target_stains = target_stains[:,:crop_size,:crop_size]\n",
    "        virtual_stains = virtual_stains[:,:crop_size,:crop_size]\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        axes[i, 0].imshow(phase_images[idx], cmap=\"gray\")\n",
    "        axes[i, 0].set_title(\"Phase\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        axes[i, 1].imshow(\n",
    "            target_stains[idx],\n",
    "            cmap=\"gray\",\n",
    "            vmin=np.percentile(target_stains[idx], 1),\n",
    "            vmax=np.percentile(target_stains[idx], 99),\n",
    "        )\n",
    "        axes[i, 1].set_title(\"Target Fluorescence \")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "        axes[i, 2].imshow(\n",
    "            virtual_stains[idx],\n",
    "            cmap=\"gray\",\n",
    "            vmin=np.percentile(target_stains[idx], 1),\n",
    "            vmax=np.percentile(target_stains[idx], 99),\n",
    "        )\n",
    "        axes[i, 2].set_title(\"Virtual Stain\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "visualise_results(phase_images, target_stains,virtual_stains)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a633df73",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0,
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.2 Compute pixel-level metrics\n",
    "\n",
    "Compute the pixel-level metrics for the virtual stains and target stains. The metrics include Pearson correlation, SSIM, and PSNR.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10ebc3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_metrics = pd.DataFrame(columns=[\"pearson_nuc\", \"SSIM_nuc\", \"psnr_nuc\"])\n",
    "# Pixel-level metrics\n",
    "for i, (target_image, predicted_image) in enumerate(zip(target_stains, virtual_stains)):\n",
    "    # Compute SSIM and pearson correlation.\n",
    "    ssim_score = metrics.structural_similarity(\n",
    "        target_image, predicted_image, data_range=1\n",
    "    )\n",
    "    pearson_score = np.corrcoef(target_image.flatten(), predicted_image.flatten())[0, 1]\n",
    "    psnr_score = metrics.peak_signal_noise_ratio(\n",
    "        target_image, predicted_image, data_range=1\n",
    "    )\n",
    "    test_metrics.loc[i] = {\n",
    "        \"pearson_nuc\": pearson_score,\n",
    "        \"SSIM_nuc\": ssim_score,\n",
    "        \"psnr_nuc\": psnr_score,\n",
    "    }\n",
    "\n",
    "test_metrics.boxplot(\n",
    "    column=[\"pearson_nuc\", \"SSIM_nuc\", \"psnr_nuc\"],\n",
    "    rot=30,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a1ca29",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "<div class=\"alert alert-info\">\n",
    "\n",
    "### Task 3.3 Compute instance-level metrics\n",
    "\n",
    "- Use Cellpose to segment the nuclei or  membrane channels of the fluorescence and virtual staining images.\n",
    "- Compute the F1 score for the segmentation masks.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c702e3a2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Run cellpose to generate masks for the virtual stains\n",
    "path_to_virtual_stain = Path(opt.results_dir)\n",
    "path_to_targets = Path(f\"{output_image_folder}/test/\")\n",
    "cellpose_model = \"nuclei\"  # or \"cyto\" depending on your choice of target for virtual stain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae3a6301",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\r\n",
      "  File \"/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/runpy.py\", line 183, in _run_module_as_main\r\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\r\n",
      "  File \"/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/runpy.py\", line 142, in _get_module_details\r\n",
      "    return _get_module_details(pkg_main_name, error)\r\n",
      "  File \"/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/runpy.py\", line 109, in _get_module_details\r\n",
      "    __import__(pkg_name)\r\n",
      "  File \"/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/site-packages/cellpose/__init__.py\", line 1, in <module>\r\n",
      "    from cellpose.version import version, version_str\r\n",
      "  File \"/hpc/user_apps/bioimaging_analytics/conda_environments/pix2pixHD_CUDA11/lib/python3.7/site-packages/cellpose/version.py\", line 5, in <module>\r\n",
      "    from importlib.metadata import PackageNotFoundError, version\r\n",
      "ModuleNotFoundError: No module named 'importlib.metadata'\r\n"
     ]
    }
   ],
   "source": [
    "# Run for virtual stain\n",
    "!python -m cellpose --dir $path_to_virtual_stain --pretrained_model $cellpose_model --chan 0 --save_tiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850cb966",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_masks = sorted([i for i in path_to_predictions.glob(\"**/*_cp_masks.tif*\")])\n",
    "target_masks = sorted([ifor i in Path('./data/nuclei/masks/).glob(\"**/*.tiff\")])\n",
    "assert len(predicted_masks) == len(target_masks), \"Number of masks do not match.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23c712",
   "metadata": {},
   "source": [
    "Use a predefined function to compute F1 score and its component parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb1e264",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataframe to store the outputs\n",
    "results = pd.DataFrame(\n",
    "    columns=[\n",
    "        'Model', 'Image', 'GT_Cell_Count','Threshold', 'F1', 'IoU',\n",
    "        'TP', 'FP', 'FN', 'Precision', 'Recall'\n",
    "    ],\n",
    ") \n",
    "# Create inputs to function\n",
    "image_sets = []\n",
    "for i in range(len(predicted_masks)):\n",
    "    name = str(predicted_masks[i]).split(\"/\")[-1] \n",
    "    virtual_stain_mask = imread(predicted_masks[i])\n",
    "    fluorescence_mask = imread(target_masks[i])  \n",
    "    image_sets.append(\n",
    "        {\n",
    "            \"Image\": name,\n",
    "            \"Model\": \"Pix2PixHD\",\n",
    "            \"Virtual_Stain_Mask\": virtal_stain_mask,\n",
    "            \"Fluorescence_Mask\": fluorescence_mask,\n",
    "        }\n",
    "    )\n",
    "# Compute the segmentation scores\n",
    "results, _, _ = \\\n",
    "    gen_segmentation_scores(\n",
    "        image_sets, results, final_score_output=f\"./GAN_code/GANs_MI2I/results/{opt.name}/results/\")\n",
    "\n",
    "results.head()\n",
    "\n",
    "# Get Mean F1 results\n",
    "mean_f1 = results[\"F1\"].mean()\n",
    "std_f1 = results[\"F1\"].std()\n",
    "print(f\"Mean F1 Score: {np.round(mean_f1,2)}\")\n",
    "\n",
    "plt.hist(results[\"F1\"], bins=10)\n",
    "plt.xlabel(\"F1 Score\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(f\"F1 Score: Mu {mean_f1}+-{std_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678620d",
   "metadata": {
    "cell_marker": "\"\"\""
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "## Checkpoint 3\n",
    "\n",
    "Congratulations! You have trained several image translation models now!\n",
    "Please document hyperparameters, snapshots of predictions on validation set, and loss curves for your models and add the final perforance in [this google doc](https://docs.google.com/document/d/1hZWSVRvt9KJEdYu7ib-vFBqAVQRYL8cWaP_vFznu7D8/edit#heading=h.n5u485pmzv2z). We\"ll discuss our combined results as a group.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95294bb",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Part 4. Visualise Regression vs Generative Modelling Approaches\n",
    "--------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0a1ce",
   "metadata": {
    "tags": [
     "task"
    ]
   },
   "outputs": [],
   "source": [
    "# Load Viscy Virtual Stains\n",
    "viscy_results_path = \"/ADD/PATH/TO/RESULTS/HERE\"\n",
    "viscy_stain_paths = sorted([i for i in Path(viscy_results_path).glob(\"**/*.tiff\")])\n",
    "assert len(viscy_stain_paths) == len(virtual_stain_paths), \"Number of images do not match.\"\n",
    "visy_stains = np.zeros((len(viscy_stain_paths), 512, 512))\n",
    "for index, v_path in enumerate(viscy_stain_paths):\n",
    "    viscy_stain = imread(v_path)\n",
    "    visy_stains[index] = viscy_stain\n",
    "\n",
    "##########################\n",
    "######## TODO ########\n",
    "##########################\n",
    "\n",
    "\n",
    "def visualise_both_methods():\n",
    "    # Your code here\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bd10f4",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "##########################\n",
    "######## Solution ########\n",
    "##########################\n",
    "\n",
    "def visualise_both_methods(\n",
    "    phase_images: np.array, target_stains: np.array, pix2pixHD_results: np.array, viscy_results: np.array,crop_size=None\n",
    "):\n",
    "    fig, axes = plt.subplots(5, 5, figsize=(15, 15))\n",
    "    sample_indices = np.random.choice(len(phase_images), 5)\n",
    "    if crop is not None:\n",
    "        phase_images = phase_images[:,:crop_size,:crop_size]\n",
    "        target_stains = target_stains[:,:crop_size,:crop_size]\n",
    "        pix2pixHD_results = pix2pixHD_results[:,:crop_size,:crop_size]\n",
    "        viscy_results = viscy_results[:,:crop_size,:crop_size]\n",
    "\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        axes[i, 0].imshow(phase_images[idx], cmap=\"gray\")\n",
    "        axes[i, 0].set_title(\"Phase\")\n",
    "        axes[i, 0].axis(\"off\")\n",
    "        axes[i, 1].imshow(\n",
    "            target_stains[idx],\n",
    "            cmap=\"gray\",\n",
    "            vmin=np.percentile(target_stains[idx], 1),\n",
    "            vmax=np.percentile(target_stains[idx], 99),\n",
    "        )\n",
    "        axes[i, 1].set_title(\"Nuclei\")\n",
    "        axes[i, 1].axis(\"off\")\n",
    "        axes[i, 2].imshow(\n",
    "            virtual_stains[idx],\n",
    "            cmap=\"gray\",\n",
    "            vmin=np.percentile(target_stains[idx], 1),\n",
    "            vmax=np.percentile(target_stains[idx], 99),\n",
    "        )\n",
    "        axes[i, 2].set_title(\"Virtual Stain\")\n",
    "        axes[i, 2].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cba488",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Part 5: BONUS: Sample different virtual staining solutions from the GAN using MC-Dropout and explore the uncertainty in the virtual stain predictions.\n",
    "--------------------------------------------------\n",
    "Steps:\n",
    "- Load the pre-trained model.\n",
    "- Generate multiple predictions for the same input image.\n",
    "- Compute the pixel-wise variance across the predictions.\n",
    "- Visualise the pixel-wise variance to explore the uncertainty in the virtual stain predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d6f712",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Use the same model and dataloaders as before.\n",
    "# Load the test data.\n",
    "test_data_loader = CreateDataLoader(opt)\n",
    "test_dataset = test_data_loader.load_data()\n",
    "visualizer = Visualizer(opt)\n",
    "\n",
    "# Load pre-trained model\n",
    "opt.variational_inf_runs = 100 # Number of samples per phase input\n",
    "opt.variation_inf_path = f\"~/data/04_image_translation/pretrained_GAN/{opt.name}/results/samples/\"  # Path to store the samples.\n",
    "opt.dropout_variation_inf = True  # Use dropout during inference.\n",
    "model = create_model(opt)\n",
    "# Generate & save predictions in the variation_inf_path directory.\n",
    "sampling(test_dataset, opt, model)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,tags,-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "pix2pixHD_CUDA11",
   "language": "python",
   "name": "pix2pixhd_cuda11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
